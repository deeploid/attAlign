# -*- coding: utf-8 -*-

from __future__ import division, print_function

"""
To train a Feature-based Attention Aligner (FAA)

Input JSON files should be generated by the script `tokenize-corpus.py`.

"""

import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as opt
from torch.autograd import Variable
import numpy as np

# Reading Arguments
parser = argparse.ArgumentParser(description='Feature-based Attention Network')
parser.add_argument('--data', type=str, default='/data',
                help='location of the data corpus')

class FAA(nn.Module):
    def __init__(self):
        super(FAA, self).__init__()
        self.layer1 = nn.Linear(300,200)

    def forward(self,x):
        x = F.relu(self.layer1(x))
        return x

    def num_flat_features(self,x):
        size = x.size()[1:] # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features
    
att = FAA()
optimizer = opt.SGD(att.parameters(), lr=0.01)
criterion = nn.MSELoss()
x = [ torch.Tensor(300, 300) ]
y = [ torch.Tensor(300, 200) ]

for epoch in range(100): # loop over the dataset multiple times
    
    running_loss = 0.0
    i = 0
    for inputs, labels in zip(x,y):
        # wrap them in Variable
        inputs, labels = Variable(inputs), Variable(labels)
        
        # zero the parameter gradients
        optimizer.zero_grad()
        
        # forward + backward + optimize
        outputs = att(inputs)
        loss = criterion(outputs, labels)
        loss.backward()        
        optimizer.step()
        
        # print statistics
        running_loss += loss.data[0]
        #if i % 2000 == 1999: # print every 2000 mini-batches
        print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss / 2000))
        running_loss = 0.0
        i += 1
print('Finished Training')





